{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"BunnyVisionPro - Overview","text":"<p>BunnyVisionPro is a robot teleoperation system integrated with Apple Vision Pro as pose tracking device, specifically tailored for bimanual dexterous manipulation. The system boasts a highly modular architecture, simplifying deployment and enhancing flexibility. Key features of the system include:</p> <ul> <li>Portability: The system only requires a Vision Pro as hand pose tracking device, making it highly portable and easy to   set up.</li> <li>Scalability: The system is designed to be compatible with a wide range of robot platforms, including xxx.</li> <li>Containization: The server and client components of the system are containerized, ensuring easy deployment and   management.</li> </ul>"},{"location":"#citing-bunnyvisionpro","title":"Citing BunnyVisionPro","text":"<p>If you find this codebase helpful in your work, please consider cite:</p> <pre><code>@article{bunny-visionpro,\n  author  = {Runyu Ding, Yuzhe Qin, Jiyue Zhu, Chengzhe Jia, Shiqi Yang, Ruihan Yang, Xiaojuan Qi, and Xiaolong Wang},\n  title   = {Bunny-VisionPro: Bimanual Dexterous Teleoperation with Real-Time Retargeting using Vision Pro},\n year   = {2024},\n}\n</code></pre>"},{"location":"#acknowledge","title":"Acknowledge","text":"<p>Our code is built upon  VisionProTeleop, dex-retargeting, sim-web-visualizer. We thank all these authors for their nicely open sourced code and their great contributions to the community.</p>"},{"location":"advanced/initialization/","title":"Teleoperation Initialization","text":"<p>Teleoperation systems map human movements to robotic actions. For instance, advancing your right hand by 0.1 meters should correspondingly move the robot's right end effector forward by the same distance. Establishing a clear definition of coordinate system for both human and robot through an initialization step is crucial to accurately translating human motion to robot in three-dimensional space.</p>"},{"location":"advanced/initialization/#calibration-between-human-and-robot","title":"Calibration Between Human and Robot","text":""},{"location":"advanced/initialization/#initialization-process","title":"Initialization Process","text":"<p>During the initialization phase, we establish a three-dimensional frame in both human and robot coordinate systems, referred to as the initial human frame and the initial robot frame, respectively. Movements by the human operator are measured relative to the initial human frame, and the robot mimics these relative movements within its corresponding initial robot frame.</p>"},{"location":"advanced/initialization/#system-configuration","title":"System Configuration","text":"<p>The positions and orientations of these initial frames are configured via the <code>TeleopClient</code> API as shown below:</p> <pre><code>import numpy as np\nfrom bunny_teleop.init_config import BimanualAlignmentMode\n\n\nclass TeleopClient:\n  def send_init_config(\n    self,\n    *,\n    robot_base_pose: Tuple[np.ndarray, np.ndarray],\n    init_qpos: Tuple[np.ndarray, np.ndarray],\n    joint_names: Tuple[List[str], List[str]],\n    align_gravity_dir=True,\n    bimanual_alignment_mode=BimanualAlignmentMode.ALIGN_CENTER,\n  ):\n    # Implementation skipped for doc purpose\n    pass\n</code></pre> <p>Checking Initialization Status</p> <p>Use <code>teleop_client.started</code> or <code>teleop_client.wait_for_server_start()</code> to confirm if the initialization has completed.</p>"},{"location":"advanced/initialization/#initializing-the-robot-frame","title":"Initializing the Robot Frame","text":"<p>Upon successful operation of the <code>bunny_teleop_server</code>, the teleop initialization is triggered by the <code>send_init_config</code> API call. This function requires the initial pose of the robot (<code>init_qpos</code>), which represents the joint positions for both the left and right robotic arms. The teleop server computes the end effector poses using forward kinematics to determine the initial robot frame.</p> <p>Be Careful about Joint Orders</p> <p>Ensure the joint names are correctly specified as the teleop server may parse the robot model differently, affecting the joint order.</p>"},{"location":"advanced/initialization/#initializing-the-human-frame","title":"Initializing the Human Frame","text":"<p>Post the API call, the VisionPro system begins tracking the operator's hand poses. To mitigate noise in hand pose estimation, an average pose is calculated from 200 frames following the initialization call. The initialization completes once these poses are processed, allowing the teleop server to begin streaming commands.</p> <p>Good Hand Poses are Important for Initialization</p> <p>The frame counter resets to 0 if the operator's hands are not held still or if the fingers are not spread out flat. If the teleop_server has not started within 5 seconds after initialization, check the position and condition of both  hands. </p> <p>An example of a proper hand pose is shown below:</p> <p></p>"},{"location":"advanced/initialization/#bimanual-alignment-modes","title":"Bimanual Alignment Modes","text":""},{"location":"advanced/initialization/#overview","title":"Overview","text":"<p>In the tutorial above, we skip how to determine the initial frames for the left and right hands. Essentially, we can treat each hand as a separate entity with its own frame, similar to individual robots, or use a single frame for controlling both hands in bimanual operations. Depending on the task, different configurations might be preferable. Our system allows users to select the desired mode through the <code>bimanual_alignment_mode</code> variable in the <code>send_init_config</code> function, offering various options to best suit specific task requirements.</p> <pre><code>class BimanualAlignmentMode(Enum):\n  ALIGN_CENTER = 0\n  ALIGN_LEFT = 1\n  ALIGN_RIGHT = 2\n  ALIGN_SEPARATELY = 3\n</code></pre>"},{"location":"advanced/initialization/#mode-descriptions","title":"Mode Descriptions","text":"<ul> <li>ALIGN_SEPARATELY: Treats each arm as an independent entity, ideal for tasks requiring distinct movements.</li> <li>ALIGN_CENTER: Calculates a center between the two robot end effectors, aligning this with the midpoint between   the human hands.</li> <li>ALIGN_LEFT and ALIGN_RIGHT: Focuses on one hand's position and orientation, useful for tasks prioritizing one   side.</li> </ul> <p>Global Pose of Robot Base</p> <p>To provide the teleoperation server with the positions of the two robot arms, the <code>robot_base_pose</code> variable is used.  This variable, a tuple consisting of two 7-dimensional numpy arrays, specifies the position and orientation (using the quaternion wxyz convention) of each robot's base.</p> <p>Danger</p> <p>For modes except <code>ALIGN_SEPARATELY</code> Robot arms may move instantly after initialization is finished even if the human operator keeps static.This can be dangers on real robot if you are not aware what will happen after initialization. Try first on our simulation version before deploying on the real robot.</p>"},{"location":"advanced/visualization/","title":"Visualization Tools","text":"<p>To provide an intuitive user interface of the initialization and teleoperation, a web-based GUI visualizes these processes in real-time. This feature utilizes the sim_web_visualizer.</p> <p>During initialization, a large 3D cursor displayed on the interface becomes increasingly transparent to reflect progress, transitioning to two smaller 3D gizmos indicating the end effector positions upon completion.</p> ALIGN_CENTER ALIGN_LEFT ALIGN_RIGHT ALIGN_SEPARATELY"},{"location":"getstarted/install/","title":"Installation","text":"<p>BunnyVisionPro is designed as a distributed system to facilitate robot teleoperation. The system consists of a server and a client that communicate via data streaming, leveraging the Apple Vision Pro as hand pose tracking device. To interact with Vision Pro, download the Tracking Streamer app on your device.</p> <p>The server can be installed and run inside a Docker container, while the client is super  light-weight can be installed using pip.</p>"},{"location":"getstarted/install/#server-installation","title":"Server Installation","text":"<p>Before running the server code inside a Docker container, you need to either pull an existing Docker image or build one yourself.</p>"},{"location":"getstarted/install/#pulling-an-existing-docker-image","title":"Pulling an Existing Docker Image","text":"<p>For quick setup, pull an existing Docker image from Docker Hub using the following command:</p> <pre><code>docker pull yzqin/bunny_teleop_server\n</code></pre>"},{"location":"getstarted/install/#building-a-docker-image-from-a-dockerfile-optional","title":"Building a Docker Image from a Dockerfile (Optional)","text":"<p>If you need a custom setup, you can also build your own Docker image using the provided Dockerfile.</p> <pre><code>git clone https://github.com/Dingry/bunny_teleop_server.git\ncd bunny_teleop_server/docker/minimal\n\ndocker build -t bunny_teleop_server .\n</code></pre>"},{"location":"getstarted/install/#client-installation","title":"Client Installation","text":"<p>The client can be easily installed using Python's pip package manager:</p> <pre><code>pip install bunny_teleop\n</code></pre>"},{"location":"getstarted/tips/","title":"Tips and Troubleshooting","text":"<ol> <li>Extend your arms forward with your palms facing downwards, maintaining a slight separation between your hands. Keep    your arms and hands in the same position until the calibration is completed.</li> <li>Start the client application for calibration between the human and robot hands with the following commands.     <pre><code>python bunny_teleop/bimanual_teleop_client.py\n</code></pre></li> </ol> <p>Note: During this process, the default setting, <code>ALIGN_SEPARATELY</code>, independently aligns each robot hand with your    respective human hand, ignoring the space between your hands. Additionally, the system automatically aligns with the    gravity direction of your hands (<code>align_gravity_dir</code> set to <code>True</code>).</p> <p>We also offer support for various initialization modes to tailor the calibration process to different operational    needs. For more detailed information on these modes, please refer to initialization.</p> <ol> <li>Once calibration is completed, you can start controlling the robots by simply moving your hands.</li> </ol>"},{"location":"getstarted/tips/#tips-on-teleoperation","title":"Tips on Teleoperation","text":"<ol> <li>When using the system for the first time, it is advisable to move your arms slowly until you get familiar with the    system. Typically, the robot arms should be posed a velocity limit, which can lead to an awareable latency. Moving    slowly can help prevent potential collisions.</li> <li>The calibration of human hand positions is set in the world coordinate system and is mostly independent of the Vision    Pro's location. Therefore, if you would like to teleoperate the robot arm to some position that is out of human    hand's action space, you can adjust your own position by stepping back, forward, left, or right, while trying to keep    your hands in their current poses to avoid unexpected movement of human hands caused by the adjustment.</li> <li>It is generally best to keep the hands parallel. However, in certain situations, an asymmetrical configuration of the    initial hand poses might be beneficial. For instance, if you need the right robot arm to reach a higher position    while maintaining a lower position with the left arm, consider raising your right hand lower than the left during the    calibration process. This is because the robot arms are always initialized at the same height. A lower initial    position of the right hand will give you more room to move your right hand upwards.</li> <li>It is recommended to fully expose all fingers under the Vision Pro's camera to ensure accurate hand tracking. If the fingers are not fully visible, the system may not be able to detect them, leading to oscillations or unexpected movements of the robot fingers.</li> </ol>"},{"location":"getstarted/tips/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>If the IP address displayed in the Tracking Streamer app is incorrect or appears as an IPv6 address, navigate to the    settings menu on your Vision Pro device: <code>Settings &gt; WiFi &gt; Details &gt; IP Configuration</code>. Ensure you obtain the    correct IPv4 address from this menu.</li> </ol>"},{"location":"getstarted/usage/","title":"Basic Usage","text":"<p>This guide covers the setup and operation of the BunnyVisionPro server and client. Although the server and client can be set up on different machines, for simplicity, this tutorial assumes they are on the same computer.</p>"},{"location":"getstarted/usage/#bunny-teleoperation-server-setup","title":"Bunny Teleoperation Server Setup","text":""},{"location":"getstarted/usage/#1-start-the-server-docker","title":"1. Start the Server Docker","text":"<p>Execute the following command to run the teleoperation server inside a Docker container:</p> <pre><code>docker run -it --net host --name bunny_teleop_server -v yzqin/bunny_teleop_server bash\n</code></pre>"},{"location":"getstarted/usage/#2-network-configuration","title":"2. Network Configuration","text":"<p>Ensure that the server, client, and Vision Pro are connected to the same Local Area Network (LAN).</p>"},{"location":"getstarted/usage/#3-launch-tracking-streamer-app","title":"3. Launch Tracking Streamer App","text":"<p>Open the Tracking Streamer app on your Vision Pro device and note the IPv4 address shown on the screen.</p>"},{"location":"getstarted/usage/#4-configure-ip-in-docker","title":"4. Configure IP in Docker","text":"<p>Update the <code>visionpro_config.yml</code> file inside the Docker container to include the IPv4 address of your Apple Vision Pro:</p> <pre><code>cameras:\n  - visionpro:\n      avp_ip: &lt;avp_ip&gt;\n</code></pre> <p>You can edit this file using a terminal-based editor such as <code>nano</code>, <code>vim</code>, or <code>emacs</code> inside docker. For example:</p> <pre><code>vim /server/bunny_teleop_server/configs/camera_config/visionpro_config.yml\n</code></pre>"},{"location":"getstarted/usage/#5-start-server-nodes-inside-docker","title":"5. Start Server Nodes Inside Docker","text":"<p>Start the vision node to stream hand pose data from VisionPro:</p> <pre><code>run_vision_server -c /server/bunny_teleop_server/configs/camera_config/visionpro_config.yml\n</code></pre> <p>Then, start the robot node for teleoperation:</p> <pre><code>run_robot_server -c /server/bunny_teleop_server/configs/camera_config/visionpro_config.yml -k /server/bunny_teleop_server/configs/kinematics_config/bimanual_xarm7_ability.yml --comm /server/bunny_teleop_server/configs/communication_config/sim_web_visualizer.yml\n</code></pre> <p>Use two terminal tabs for these servers, which can be managed using <code>tmux</code> inside Docker or by opening another terminal tab with:</p> <pre><code>docker exec -it bunny_teleop_server bash\n</code></pre>"},{"location":"getstarted/usage/#6-web-visualization","title":"6. Web Visualization","text":"<p>Open a web browser and navigate to http://127.0.0.1:7000/static/. You should see the teleoperation interface with both left and robot arms overlapped at the world origin. This is because the teleop server will not know the pose of left and right robot base in the world coordinate until the client informs it.</p> <p></p> <p>Tip</p> <p>To access the web visualization from a different machine, replace <code>127.0.0.1</code> with the IP address of the machine running the teleop server.</p>"},{"location":"getstarted/usage/#bunny-teleoperation-client-setup","title":"Bunny Teleoperation Client Setup","text":""},{"location":"getstarted/usage/#starting-the-minimal-teleop-client","title":"Starting the Minimal Teleop Client","text":"<p>To initiate the teleoperation client, follow these steps:</p> <pre><code>git clone https://github.com/Dingry/BunnyVisionPro\ncd BunnyVisionPro\npython examples/minimal/minimal.py\n</code></pre> <p>Upon starting the teleop client, it immediately transmits robot information to the server. You should then observe the initial state on the web visualization interface:</p> <p></p> <p>More information about initialization can be found in Teleoperation Initialization</p> <p>Note</p> <p>Once the client is running, if the hand pose is accurately detected, the initialization completes quickly, and the robots will begin to mimic your hand movements. You may not have a chance to the see the figure above. Don't worry, this indicates that the system is functioning correctly.</p>"},{"location":"system/overview/","title":"System Overview","text":""}]}